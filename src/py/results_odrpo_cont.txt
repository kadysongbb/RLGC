{"train_rewards": [[1, -858.1832885742188], [101, -3076.6832885742188], [201, -3076.683303833008], [301, -2218.5], [401, -2218.5], [501, -2218.500030517578], [601, -2227.5], [701, -2227.5], [801, -3076.6832580566406], [901, -3076.6832580566406], [1001, -2227.4999771118164], [1101, -2221.499969482422], [1201, -2212.5000381469727], [1301, -2227.5], [1401, -2212.4999809265137], [1501, -2227.5], [1601, -2218.5], [1701, -3076.6832885742188], [1801, -2215.5000343322754], [1901, -2215.499984741211], [2001, -2215.4999599456787], [2101, -2218.499969482422], [2201, -2212.499969482422], [2301, -2215.499973297119], [2401, -2218.500030517578], [2501, -2209.500045776367], [2601, -2212.499984741211], [2701, -2218.5], [2801, -3070.683265686035], [2901, -2212.4999923706055], [3001, -3067.683298110962], [3101, -2221.5], [3201, -3070.6832580566406], [3301, -3067.683265686035], [3401, -2212.499969482422], [3501, -2218.499988555908], [3601, -2206.5], [3701, -3064.6832580566406], [3801, -3052.6833000183105], [3901, -3040.683298587799]], "eval_rewards": [{"total_steps": 0, "train_steps": 0, "average_eval_reward": -2307.0183197021483, "eval_reward_variance": 11660.189534178668}], "actor_losses": [], "value_losses": [], "critic_losses": []}