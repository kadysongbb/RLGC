{"train_rewards": [[1, -858.1832885742188], [101, -3076.683349609375], [201, -3076.6832580566406], [301, -2218.500030517578], [401, -2218.500030517578], [501, -2218.499954223633], [601, -2227.5], [701, -2227.5], [801, -3076.6832275390625], [901, -3076.6832733154297], [1001, -2227.5], [1101, -2218.500015258789], [1201, -2212.499969482422], [1301, -2227.5], [1401, -2212.5], [1501, -2224.4999771118164], [1601, -2215.5000190734863], [1701, -3073.6833457946777], [1801, -2218.5], [1901, -2215.499988555908], [2001, -2218.5], [2101, -2218.5], [2201, -2206.4999809265137], [2301, -2215.4999866485596], [2401, -2215.5], [2501, -2197.499976158142], [2601, -2179.5000081062317], [2701, -2197.499984741211], [2801, -3028.6832885742188], [2901, -2194.499988555908], [3001, -3034.683274745941], [3101, -2176.5000076293945], [3201, -3028.6832885742188], [3301, -3028.683292388916], [3401, -2185.5000381469727], [3501, -2188.499984741211], [3601, -2176.4999961853027], [3701, -3007.6832885742188], [3801, -3010.6832885742188], [3901, -3025.6832942962646]], "eval_rewards": [{"total_steps": 0, "train_steps": 0, "average_eval_reward": -2307.0183197021483, "eval_reward_variance": 11649.354901124909}], "actor_losses": [], "value_losses": [], "critic_losses": []}