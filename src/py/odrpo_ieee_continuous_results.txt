{"train_rewards": [[1, -858.1832885742188], [101, -3076.683319091797], [201, -3076.6832427978516], [301, -2218.499969482422], [401, -2218.500030517578], [501, -2218.499969482422], [601, -2227.5], [701, -2227.4999389648438], [801, -3076.6832580566406], [901, -3076.6832885742188], [1001, -2227.500045776367], [1101, -2224.5], [1201, -2212.4999656677246], [1301, -2227.5], [1401, -2212.4999961853027], [1501, -2227.5], [1601, -2218.5], [1701, -3076.6832885742188], [1801, -2218.499969482422], [1901, -2212.4999961853027], [2001, -2218.5], [2101, -2221.5], [2201, -2212.499969482422], [2301, -2215.499984741211], [2401, -2218.499984741211], [2501, -2212.500030517578], [2601, -2212.499984741211], [2701, -2221.499984741211], [2801, -3070.683292388916], [2901, -2212.5], [3001, -3061.6832580566406], [3101, -2203.4999504089355], [3201, -3040.6832671165466], [3301, -2995.683280944824], [3401, -2056.500030517578], [3501, -1987.4999866485596], [3601, -1963.499994277954], [3701, -2807.1501569747925], [3801, -3057.1229572296143], [3901, -2794.7755308151245], [4001, -2843.729389190674], [4101, -1960.4999694824219], [4201, -2839.683313369751], [4301, -2858.6151065826416], [4401, -1990.4999828338623], [4501, -1972.500005722046], [4601, -2140.5000133514404], [4701, -2146.5], [4801, -2158.4999771118164], [4901, -2176.499969482422], [5001, -2173.4999866485596], [5101, -3034.683292388916], [5201, -2182.5], [5301, -2185.5000228881836], [5401, -3025.6833057403564], [5501, -2179.4999961853027], [5601, -2188.5], [5701, -3058.6832885742188], [5801, -2185.500020980835], [5901, -2218.5], [6001, -3055.6832885742188], [6101, -3052.6832885742188], [6201, -2218.5], [6301, -2194.5], [6401, -2206.500015258789], [6501, -2209.499984741211], [6601, -2191.5000228881836], [6701, -2215.5], [6801, -2200.4999923706055], [6901, -2203.4999923706055], [7001, -2194.5000190734863], [7101, -3058.6832885742188], [7201, -2197.4999771118164], [7301, -2197.499988555908], [7401, -2203.4999771118164]], "eval_rewards": [{"total_steps": 0, "train_steps": 0, "average_eval_reward": -2307.0183502197265, "eval_reward_variance": 12107.288266744485}], "actor_losses": [], "value_losses": [], "critic_losses": []}